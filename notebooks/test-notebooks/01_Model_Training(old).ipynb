{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if missing (uncomment if needed)\n",
    "# %pip install tensorflow pandas numpy scikit-learn joblib matplotlib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, RepeatVector, TimeDistributed, Input\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress TensorFlow warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "data_dir = '../data/processed/'\n",
    "ag_df = pd.read_csv(os.path.join(data_dir, 'data_set_AG_1.csv'))\n",
    "mal_df = pd.read_csv(os.path.join(data_dir, 'data_set_MAL_1.csv'))\n",
    "# set train/test from the processed CSVs\n",
    "train = ag_df.copy()\n",
    "test = mal_df.copy()\n",
    "train.shape, test.shape\n",
    "\n",
    "# for filename in os.listdir(data_dir):\n",
    "#     dataset = pd.read_csv(os.path.join(data_dir, filename), sep='\\t')\n",
    "#     dataset_mean_abs = np.array(dataset.abs().mean())\n",
    "#     dataset_mean_abs = pd.DataFrame(dataset_mean_abs.reshape(1,4))\n",
    "#     dataset_mean_abs.index = [filename]\n",
    "#     # train = train.append(dataset_mean_abs)\n",
    "#     train = pd.concat([train, dataset_mean_abs])\n",
    "    \n",
    "# train.columns = ['Bearing 1', 'Bearing 2', 'Bearing 3', 'Bearing 4']\n",
    "# #train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features (excluding non-numeric cols like 'type', 'sender', 'to')\n",
    "numeric_cols = ['approveAmount', 'transferAmount', 'transferFromAmount', \n",
    "                'oldApproveState', 'newApproveState', 'oldBalanceState', \n",
    "                'newBalanceState', 'success'] # 'success' is boolean, convert to int\n",
    "\n",
    "# Clean Data\n",
    "train_data = ag_df[numeric_cols].copy()\n",
    "train_data['success'] = train_data['success'].astype(int)\n",
    "\n",
    "# Normalize Data (Crucial for Neural Networks)\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(train_data)\n",
    "print(f\"Data Scaled. Shape: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_cumm = dict()\n",
    "print(from_cumm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_replace = {\"transfer\": float(0), \"transferFrom\": float(1), \"approve\": float(2)}\n",
    "train_type_replaced = train.replace(type_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_type_replaced.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_type_replaced.drop(columns=['to', 'sender'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform data file index to datetime and sort in chronological order\n",
    "# train.index = pd.to_datetime(train.index, format='%Y.%m.%d.%H.%M.%S')\n",
    "# train = train.sort_index()\n",
    "# train.to_csv('Averaged_BearingTest_Dataset.csv')\n",
    "# print(\"Dataset shape:\", train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 6), dpi=80)\n",
    "ax.plot(train['type'], label='type', color='blue', animated = True, linewidth=1)\n",
    "ax.plot(train['approveAmount'], label='approveAmount', color='red', animated = True, linewidth=1)\n",
    "ax.plot(train['transferAmount'], label='transferAmount', color='green', animated = True, linewidth=1)\n",
    "ax.plot(train['transferFromAmount'], label='transferFromAmount', color='black', animated = True, linewidth=1)\n",
    "plt.legend(loc='lower left')\n",
    "# ax.set_title('Bearing Sensor Training Data', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming data from the time domain to the frequency domain using fast Fourier transform\n",
    "# train_fft = np.fft.fft(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(14, 6), dpi=80)\n",
    "# ax.plot(train_fft[:,0].real, label='Bearing 1', color='blue', animated = True, linewidth=1)\n",
    "# ax.plot(train_fft[:,1].imag, label='Bearing 2', color='red', animated = True, linewidth=1)\n",
    "# ax.plot(train_fft[:,2].real, label='Bearing 3', color='green', animated = True, linewidth=1)\n",
    "# ax.plot(train_fft[:,3].real, label='Bearing 4', color='black', animated = True, linewidth=1)\n",
    "# plt.legend(loc='lower left')\n",
    "# ax.set_title('Bearing Sensor Training Frequency Data', fontsize=16)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the data\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(train)\n",
    "# X_test = scaler.transform(test)\n",
    "scaler_filename = \"scaler_data\"\n",
    "joblib.dump(scaler, scaler_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequence(data, sequence_length):\n",
    "\tseq_data = []\n",
    "\t# print(seq_data)\n",
    "\tfor i in range(len(data)- sequence_length+1):\n",
    "\t\tseq = data[ i : i+sequence_length ]\n",
    "\t\t# print(seq)\n",
    "\t\tseq_data.append(seq)\n",
    "\n",
    "\t# print(seq_data)\n",
    "\treturn np.array(seq_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 20\n",
    "\n",
    "a = generate_sequence(X_train, SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Input, Dense, RepeatVector, TimeDistributed\n",
    "CuDNNLSTM = LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape=(a.shape[1], a.shape[2])))\n",
    "model.add(CuDNNLSTM(64, return_sequences=True  ))\n",
    "model.add(CuDNNLSTM(SEQUENCE_LENGTH, return_sequences=False ))\n",
    "model.add(Dense(SEQUENCE_LENGTH))\n",
    "model.add(RepeatVector(SEQUENCE_LENGTH))\n",
    "model.add(CuDNNLSTM(SEQUENCE_LENGTH, return_sequences=True ))\n",
    "model.add(CuDNNLSTM(64, return_sequences=True  ))\n",
    "model.add(TimeDistributed(Dense(a.shape[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mae')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(a, a, epochs=3, batch_size=1, validation_split=0.05, ).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 6), dpi=80)\n",
    "ax.plot(history['loss'], 'b', label='Train', linewidth=2)\n",
    "ax.plot(history['val_loss'], 'r', label='Validation', linewidth=2)\n",
    "ax.set_title('Model loss', fontsize=16)\n",
    "ax.set_ylabel('Loss (mae)')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the already-loaded `mal_df` from processed data\n",
    "test = mal_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_type_replaced = test.replace(type_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_type_replaced.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test_type_replaced.drop(columns=['to', 'sender'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = scaler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = generate_sequence(X_test, SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_pred = model.predict(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = np.mean(np.abs(b_pred-b), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_loss_b = np.sum(loss, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# natural numbering of the bearings\n",
    "# BEARINGNUMBER = 1\n",
    "# BEARINGNUMBER_WHOLE = BEARINGNUMBER - 1\n",
    "fig, ax = plt.subplots(figsize=(14, 6), dpi=80)\n",
    "\n",
    "ax.plot(combined_loss_b, 'r', label=f'combined_loss_b', linewidth=1)\n",
    "\n",
    "ax.set_title('malicious data reconstruction loss', fontsize=16)\n",
    "ax.set_ylabel('Loss (mae)')\n",
    "ax.set_xlabel('TXn')\n",
    "ax.legend(loc='upper right')\n",
    "# plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_pred = model.predict(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_a = np.mean(np.abs(a_pred - a), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_loss_a = np.sum(loss_a, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_a[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# natural numbering of the bearings\n",
    "# BEARINGNUMBER = 1\n",
    "# BEARINGNUMBER_WHOLE = BEARINGNUMBER - 1\n",
    "fig, ax = plt.subplots(figsize=(14, 6), dpi=80)\n",
    "\n",
    "ax.plot(combined_loss_a, 'g', label=f'combined_loss_a', linewidth=1)\n",
    "\n",
    "ax.set_title('good data reconstruction loss', fontsize=16)\n",
    "ax.set_ylabel('Loss (mae)')\n",
    "ax.set_xlabel('TXn')\n",
    "ax.legend(loc='upper right')\n",
    "# plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 6), dpi=80)\n",
    "\n",
    "ax.plot(combined_loss_a, 'g', label=f'Benign Transactions', linewidth=1)\n",
    "ax.plot(combined_loss_b, 'r', label=f'Malicious Transactions', linewidth=1)\n",
    "\n",
    "ax.set_title('Malicious vs Benign reconstruction loss - sequence length=20', fontsize=16)\n",
    "ax.set_ylabel('Loss (mae)')\n",
    "ax.set_xlabel('TXn')\n",
    "ax.legend(loc='upper right')\n",
    "# plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import save_model\n",
    "import joblib\n",
    "\n",
    "# Save Model\n",
    "model.save('../models/fraud_detection_model.h5')\n",
    "\n",
    "# Save Scaler (If you re-trained the scaler)\n",
    "# joblib.dump(scaler, '../models/scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# --- 1. DETERMINE THRESHOLD ---\n",
    "# We use the MAX loss of the training data (benign) as the baseline threshold.\n",
    "# Any transaction with error higher than this is \"Anomalous\".\n",
    "X_train_pred = model.predict(X_train_seq)\n",
    "train_mae_loss = np.mean(np.abs(X_train_pred - X_train_seq), axis=1)\n",
    "threshold = np.mean(train_mae_loss) + (2 * np.std(train_mae_loss)) # Standard statistical threshold (Mean + 2*SD)\n",
    "print(f\"Calculated Threshold: {threshold:.4f}\")\n",
    "\n",
    "# --- 2. EVALUATE ON \"TEST\" DATA (Malicious + Benign Mixed) ---\n",
    "# Let's create a mixed test set to see how well it catches fraud\n",
    "# (In a real scenario, you would keep a separate test set, but for this demo, we mix them)\n",
    "\n",
    "# Create sequences for Malicious data\n",
    "X_mal = scaler.transform(mal_df[numeric_cols])\n",
    "X_mal_seq = create_sequences(X_mal, SEQUENCE_LENGTH)\n",
    "\n",
    "# Ground Truth: 0 = Benign, 1 = Fraud\n",
    "# We take a slice of benign data and all malicious data\n",
    "n_benign = len(X_train_seq)\n",
    "n_malicious = len(X_mal_seq)\n",
    "\n",
    "# Combine Real Data\n",
    "X_test_seq = np.concatenate([X_train_seq, X_mal_seq])\n",
    "y_true = np.concatenate([np.zeros(n_benign), np.ones(n_malicious)])\n",
    "\n",
    "# --- 3. RUN PREDICTION ---\n",
    "X_test_pred = model.predict(X_test_seq)\n",
    "test_mae_loss = np.mean(np.abs(X_test_pred - X_test_seq), axis=1)\n",
    "\n",
    "# Logic: If Loss > Threshold -> Prediction is 1 (Fraud)\n",
    "y_pred = [1 if e > threshold else 0 for e in np.mean(test_mae_loss, axis=1)]\n",
    "\n",
    "# --- 4. PRINT METRICS ---\n",
    "print(\"\\n--- FINAL MODEL PERFORMANCE ---\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_true, y_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_true, y_pred):.4f}\")\n",
    "print(f\"Recall:    {recall_score(y_true, y_pred):.4f}\")\n",
    "print(f\"F1-Score:  {f1_score(y_true, y_pred):.4f}\")\n",
    "\n",
    "# --- 5. PLOT CONFUSION MATRIX ---\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Benign', 'Fraud'], yticklabels=['Benign', 'Fraud'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Class')\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
